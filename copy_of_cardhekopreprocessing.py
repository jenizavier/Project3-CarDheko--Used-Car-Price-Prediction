# -*- coding: utf-8 -*-
"""Copy of cardhekopreprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AAPFTK9l9o7CbuMd5j0ljJoYJ0F5Eo_t
"""

import sklearn
print(sklearn.__version__)

import pandas as pd
df=pd.read_pickle("/content/concatenated_cars.pkl")
df.head(5)

df["City"].unique()

df["ft"].unique()

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

df.isnull().sum()/len(df)*100

#irrelevant for price prediction and redundant columns been dropped
df.drop(['it', 'owner', 'centralVariantId', 'variantName', 'trendingText.imgUrl',
         'trendingText.heading', 'trendingText.desc', 'Kms Driven', 'priceActual',
         'Ownership', 'RTO', 'Registration Year', 'Seats', 'BoreX Stroke',
         'Year of Manufacture', 'Engine', 'Engine Displacement', 'Max Torque'],
        axis=1, inplace=True)

# Calculate the percentage of missing values for each column
df_null= df.isnull().mean() * 100

# Find columns with more than 80% missing values
columns_with_nulls = df_null[df_null > 80].index

# Display the columns
print("Columns with more than 80% missing values:")
print(columns_with_nulls)

df.drop(['priceSaving', 'priceFixedText', 'Ground Clearance Unladen'],axis=1,inplace=True)

df.shape

# Check the number of duplicate rows
num_duplicates = df.duplicated().sum()
print(f"Number of duplicate rows: {num_duplicates}")

# Remove duplicate rows and save back to the same DataFrame
df.drop_duplicates(inplace=True)

# Verify that duplicates are removed
new_num_duplicates = df.duplicated().sum()
print(f"Number of duplicate rows after cleaning: {new_num_duplicates}")

# Display the cleaned DataFrame
df.reset_index(drop=True, inplace=True)
pd.set_option('display.max_columns', None)  # View all columns
df.head(2)

df.shape

import pandas as pd
import re

# Function to handle missing values
def handle_missing_values(df):
    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            skewness = df[col].skew()
            if -0.4 <= skewness <= 0.4:
                df[col].fillna(df[col].mean(), inplace=True)
            else:
                df[col].fillna(df[col].median(), inplace=True)
        else:
            df[col].fillna(df[col].mode()[0], inplace=True)
    return df

# Apply handle_missing_values function to DataFrame
df = handle_missing_values(df)

# Clean column names
df.columns = df.columns.str.strip()

# Verify column names
print("Columns in DataFrame:")
print(df.columns)

# Function to clean and convert price to numeric
def convert_price(val):
    if isinstance(val, str):
        cleaned_val = re.sub(r'[â‚¹,]', '', val).strip()
        if 'Lakh' in cleaned_val:
            cleaned_val = cleaned_val.replace('Lakh', '').strip()
            if cleaned_val:
                return float(cleaned_val) * 100000
        elif 'Crore' in cleaned_val:
            cleaned_val = cleaned_val.replace('Crore', '').strip()
            if cleaned_val:
                return float(cleaned_val) * 10000000
        if cleaned_val:
            return float(cleaned_val)
    return val

# Function to extract digits from strings
def extract_digits(val):
    if isinstance(val, str):
        return re.sub(r'[^\d.]', '', val)  # Keep digits and decimal points
    return val

# List of columns to apply extraction to
columns_to_extract = [
    'Acceleration', 'km', 'Cargo Volumn', 'Front Tread', 'Gear Box',
    'Gross Weight', 'Height', 'Kerb Weight', 'Length', 'Max Power', 'Mileage',
    'Rear Tread', 'Top Speed', 'Torque', 'Turning Radius', 'Wheel Base', 'Width'
]

# Apply price conversion
df['price'] = df['price'].apply(convert_price)

# Check for missing columns
missing_cols = [col for col in columns_to_extract if col not in df.columns]
if missing_cols:
    print(f"Missing columns: {missing_cols}")

# Apply extraction to all specified columns
for col in columns_to_extract:
    if col in df.columns:
        df[col] = df[col].apply(lambda x: extract_digits(x))
    else:
        print(f"Column '{col}' not found in DataFrame.")

# Remove ':1' from 'Compression Ratio' and convert to float
if 'Compression Ratio' in df.columns:
    df['Compression Ratio'] = df['Compression Ratio'].str.replace(':1', '', regex=False)

# Convert all columns to numeric (either int or float)
for col in columns_to_extract + ['Compression Ratio', 'price']:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to int/float and handle errors
    else:
        print(f"Column '{col}' not found in DataFrame.")

# Display the updated DataFrame
pd.set_option('display.max_columns', None)  # Ensure all columns are displayed
print(df.head(2))

df.shape

df.head(2)

# Define a function to clean and convert columns
def clean_numeric(column):
    # Extract numeric values from strings
    df[column] = df[column].str.extract(r'(\d+\.?\d*)')[0].astype(float)

# Apply the function to relevant columns
columns_to_clean = [
    'Alloy Wheel Size',
    'Displacement',
    'Seating Capacity',
    'Wheel Size',
    'No Door Numbers'
]

for col in columns_to_clean:
    clean_numeric(col)

print(df.head(2))

df.info()

df2=df.copy()

df2.columns

from google.colab import files

# Create a copy of the DataFrame (example)
df2 = df.copy()

# Save DataFrame to a CSV file
df2.to_csv('df2.csv', index=False)

# Download the CSV file
files.download('df2.csv')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample DataFrame df
# df = pd.read_csv('your_dataset.csv')  # Replace with your dataset

# List of numeric columns to check for outliers
numeric_columns = [
    'price', 'km', 'Acceleration', 'Alloy Wheel Size', 'Cargo Volumn',
    'Compression Ratio', 'Displacement', 'Gross Weight', 'Height',
    'Kerb Weight', 'Length', 'Max Power', 'Mileage', 'Torque',
    'Top Speed', 'Turbo Charger', 'Turning Radius', 'Wheel Size', 'Width'
]

# Exclude 'Compression Ratio' for plotting
numeric_columns_to_plot = [col for col in numeric_columns if col != 'Compression Ratio']

# Check if columns exist in the DataFrame
existing_columns = [col for col in numeric_columns_to_plot if col in df.columns]

# Plot boxplots before removing outliers
plt.figure(figsize=(16, 12))
for i, col in enumerate(existing_columns, 1):
    plt.subplot(4, 5, i)  # Adjust subplot grid size as needed
    sns.boxplot(df[col])
    plt.title(f'Before: {col}')
plt.tight_layout()
plt.show()

# Detect outliers using IQR and apply capping/flooring
for col in existing_columns:
    if df[col].dtype in [np.int64, np.float64]:  # Apply only to numeric columns
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Cap the outliers
        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])

# Drop columns with a single unique value
columns_to_drop = [col for col in df.columns if df[col].nunique() == 1]
df = df.drop(columns=columns_to_drop)

# Plot boxplots after removing outliers
plt.figure(figsize=(16, 12))
for i, col in enumerate(existing_columns, 1):
    plt.subplot(4, 5, i)  # Adjust subplot grid size as needed
    sns.boxplot(df[col])
    plt.title(f'After: {col}')
plt.tight_layout()
plt.show()

# Display the cleaned DataFrame and dropped columns
print("Dropped columns:", columns_to_drop)
print(f"Remaining rows: {df.shape[0]}")
print(df.head(2))

df.isnull().sum()/len(df)*100

#numerical columns, so you might consider using the median if the data is skewed, or the mean if it is normally distributed.
df['Kerb Weight'].fillna(df['Kerb Weight'].median(), inplace=True)
df['Top Speed'].fillna(df['Top Speed'].median(), inplace=True)
df['Turning Radius'].fillna(df['Turning Radius'].median(), inplace=True)
df['Compression Ratio'].fillna(df['Compression Ratio'].mean(), inplace=True)
#Since this percentage is still relatively low, you can impute using the mode (most common value), especially since Gear Box is likely a categorical feature.
df['Gear Box'].fillna(df['Gear Box'].mode()[0], inplace=True)

df.isnull().sum()/len(df)*100

df.duplicated().sum()

df.drop_duplicates(inplace=True)

#encoding toconvert categorical to numeric values for statistical analysis
from sklearn.preprocessing import LabelEncoder
enc = LabelEncoder()

for i in df.select_dtypes(include="object").columns:
    df[i]=enc.fit_transform(df[i])

df.info()

df.head(10)

df['City'].unique()

#df.drop(['Compression Ratio'],axis=1,inplace=True)

df.shape



df.corr()

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(30, 26))
sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap='coolwarm', vmin=-1, vmax=1, cbar=True)

plt.xticks(rotation=45, ha='right', fontsize=14)  # Rotate x labels
plt.yticks(rotation=0, fontsize=14)  # Rotate y labels
# Add a title
plt.title('Correlation Heatmap')

# Display the heatmap
plt.show()

df.head(2)

#Statistical analysis
from scipy import stats
cont = []
cat = []
for i in df.columns:
  if df[i].nunique()>10:
    cont.append(i)
  else:
    cat.append(i)

print(cont)
print(cat)

df1=df.copy()

def two_sample(d1,d2):
  m =[0,0]
  for i in range(31):
    sample1 = d1.sample(frac=0.03)
    sample2 = d2.sample(frac=0.04)
    t_test, p_value = stats.ttest_ind(sample1,sample2)
    if p_value < 0.04:
      #print("H0 Reject null hypothesis, Ha accepts alternate hypothesis ,  data is not normally distributed")
      m[1]+=1
    else:
      #print("H0 accept null hypothesis, Ha rejects alternate hypothesis,  data is normally distributed")
      m[0]+=1
    if m[0]>m[1]:
      return True
    elif m[0]<m[1]:
      return False

def chi_square(d1,d2):
  return True if stats.chi2_contingency(pd.crosstab(d1,d2))[1]<0.075 else False

def annova(d1,d2): #assume d1 - continous, d2 - category
  group = df1[d2].unique()

  data1 = {}
  for i in group:
    data1[i]=df[d1][df[d2]==i]


  f_value , p_value = stats.f_oneway(*[i for i in data1.values()])
  if p_value<0.075:
    #print("H0 Reject null hypothesis, Ha accepts alternate hypothesis , no connection of the data")
    return False
  else:
    #print("H0 accept null hypothesis, Ha rejects alternate hypothesis, connection present in the data")
    return True

final = {}
for i in df.columns:
  final[i] = {}
  for j in df.columns:

    if i in cont and j in cont:
      result = two_sample(df[i],df[j])

    elif i in cat and j in cat:
      result = chi_square(df[i],df[j])

    elif i in cont and j in cat:
      result = annova(i,j)

    elif i in cat and j in cont:
      result = annova(j,i)
    if result :
      final[i][j]=1
    else:
      final[i][j]=0

final_df = pd.DataFrame(final)
final_df.head(2)

final_df['Compression Ratio'].nunique()

#hypotheses testing
import seaborn as sns
plt.figure(figsize=(30, 26))
sns.heatmap(final_df,cmap="coolwarm",annot=True)
plt.xticks(rotation=45, ha='right', fontsize=14)  # Rotate x labels
plt.yticks(rotation=0, fontsize=14)  # Rotate y labels
# Add a title
plt.title('testing')

# Display the heatmap
plt.show()

#to find feature importance using random forest regressor (PRICE CONTINUOUS)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

print("Scikit-learn version:", sklearn.__version__)
# Assuming df is your DataFrame and 'price' is your target variable
X = df.drop('price', axis=1)
y = df['price']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Get feature importances
importances = model.feature_importances_
features = X.columns  # Get feature names from the DataFrame

# Create a DataFrame for feature importances
feature_importances = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importances')
plt.show()

# Define the number of top features to select
top_n = 15  # Change this to the desired number of top features

# Select the top 'n' features based on importance
top_features = feature_importances.head(top_n)['Feature']

# Filter the original dataset to keep only the top features
X_reduced = X[top_features]

# Print the selected features
print("Selected features based on importance:")
print(top_features)

# Optionally, you can also visualize the selected features
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importances.head(top_n))
plt.title('Top 15 Feature Importances')
plt.show()

df.head(2)

#3) Model Development(since target is continuous regression used to develop ML)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
# Print the version of scikit-learn
print("Scikit-learn version:", sklearn.__version__)
# Assuming 'X' is the original feature set and 'y' is the target variable
# X_reduced is the dataset with the top selected features

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=42)

# Initialize models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42)
}

# Train and evaluate models
results = {}
for name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Calculate performance metrics
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store results
    results[name] = {
        "MSE": mse,
        "R2 Score": r2
    }

# Display results
results_df = pd.DataFrame(results).T
print(results_df)

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

# Define the corrected parameter grid for Random Forest
rf_param_grid = {
    'n_estimators': [50, 100, 200],          # Number of trees in the forest
    'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree
    'max_features': ['sqrt', 'log2', None],  # Number of features to consider
    'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node
    'bootstrap': [True, False]               # Whether bootstrap samples are used
}
# Print the version of scikit-learn
print("Scikit-learn version:", sklearn.__version__)
# Initialize the RandomizedSearchCV object for Random Forest
rf_random_search = RandomizedSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_distributions=rf_param_grid,
    n_iter=50,                            # Number of parameter settings to sample
    cv=5,                                # 5-fold cross-validation
    scoring='r2',                        # Use RÂ² score for evaluation
    n_jobs=-1,                           # Use all available cores
    verbose=2,                           # Print detailed output
    random_state=42                      # For reproducibility
)

# Fit the model
rf_random_search.fit(X_train, y_train)

# Get the best parameters and the corresponding RÂ² score
print("Best parameters for Random Forest:", rf_random_search.best_params_)
print("Best R2 score for Random Forest:", rf_random_search.best_score_)

# Evaluate the best model on the test set
best_rf_model = rf_random_search.best_estimator_
y_pred_rf = best_rf_model.predict(X_test)
rf_mse = mean_squared_error(y_test, y_pred_rf)
rf_r2 = r2_score(y_test, y_pred_rf)

print(f"Tuned Random Forest MSE: {rf_mse}")
print(f"Tuned Random Forest R2 Score: {rf_r2}")

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
from sklearn.metrics import mean_squared_error, r2_score
# Print the version of scikit-learn
print("Scikit-learn version:", sklearn.__version__)
# Define the parameter distributions for Gradient Boosting
gb_param_dist = {
    'n_estimators': randint(50, 200),            # Number of boosting stages to be run
    'learning_rate': uniform(0.01, 0.2),         # Learning rate shrinks the contribution of each tree
    'max_depth': randint(3, 10),                 # Maximum depth of the individual trees
    'min_samples_split': randint(2, 10),          # Minimum number of samples required to split an internal node
    'min_samples_leaf': randint(1, 4),            # Minimum number of samples required to be at a leaf node
    'subsample': uniform(0.8, 0.2)                # Fraction of samples used to fit each tree
}

# Initialize the RandomizedSearchCV object for Gradient Boosting
gb_random_search = RandomizedSearchCV(
    estimator=GradientBoostingRegressor(random_state=42),
    param_distributions=gb_param_dist,
    n_iter=30,                # Number of parameter settings to sample
    cv=5,                     # 5-fold cross-validation
    scoring='r2',             # Use RÂ² score for evaluation
    n_jobs=-1,                # Use all available cores
    verbose=2,                # Print detailed output
    random_state=42           # Ensure reproducibility
)

# Fit the model
gb_random_search.fit(X_train, y_train)

# Get the best parameters and the corresponding RÂ² score
print("Best parameters for Gradient Boosting:", gb_random_search.best_params_)
print("Best R2 score for Gradient Boosting:", gb_random_search.best_score_)

# Evaluate the best model on the test set
best_gb_model = gb_random_search.best_estimator_
y_pred_gb = best_gb_model.predict(X_test)
gb_mse = mean_squared_error(y_test, y_pred_gb)
gb_r2 = r2_score(y_test, y_pred_gb)

print(f"Tuned Gradient Boosting MSE: {gb_mse}")
print(f"Tuned Gradient Boosting R2 Score: {gb_r2}")

import joblib
from sklearn.ensemble import GradientBoostingRegressor

joblib.dump(model, 'tuned_gradient_boosting_model.pkl')

from google.colab import files

# Download the file
files.download('tuned_gradient_boosting_model.pkl')

#model comparison to find best model

import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# Assuming X_train, X_test, y_train, y_test are already defined
# Assuming the tuned models have been fitted already:
# best_rf_model for RandomForestRegressor
# best_gb_model for GradientBoostingRegressor

# Models dictionary including the tuned models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest (Tuned)": best_rf_model,
    "Gradient Boosting (Tuned)": best_gb_model
}

# To store the evaluation results
results = {}

for name, model in models.items():
    # Fit the model if it's not tuned
    if 'Tuned' not in name:
        model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Calculate performance metrics
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store results
    results[name] = {
        "MAE": mae,
        "MSE": mse,
        "R2 Score": r2
    }

# Convert results to DataFrame for better visualization
results_df = pd.DataFrame(results).T

# Print the results for comparison
print(results_df)

import sklearn
print(sklearn.__version__)

!pip install scikit-learn==1.5.1